Here is a **clean, polished, GitHub-ready README** with badges and professional layout â€” **all in one copy-paste block** just like before:

---

```markdown
# ğŸ§ Sound Classification ML Project  
![Python](https://img.shields.io/badge/Python-3.10+-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-Backend-green)
![License](https://img.shields.io/badge/License-MIT-yellow)
![Status](https://img.shields.io/badge/Status-Active-success)

A machine learning project that classifies short audio clips (beep, clap, noise, etc.) using **Python**, **Librosa**, **Scikit-Learn**, and **FastAPI**.  
Includes a full backend API and a reusable ML pipeline.

---

## ğŸ“ Project Structure

```

speechRecognition/
â”‚
â”œâ”€â”€ dataset/               # Training audio dataset
â”‚   â”œâ”€â”€ beep/
â”‚   â”œâ”€â”€ clap/
â”‚   â””â”€â”€ noise/
â”‚
â”œâ”€â”€ model/
â”‚   â””â”€â”€ sound_model.pkl    # Saved ML model (after training)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract_features.py
â”‚   â”œâ”€â”€ train_model.py
â”‚   â””â”€â”€ predict.py
â”‚
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py            # FastAPI server
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

# âš™ï¸ Installation & Setup

## 1ï¸âƒ£ Create a Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate      # Linux / macOS
venv\Scripts\activate         # Windows
````

---

## 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### The requirements file should contain:

```
librosa
numpy
scikit-learn
soundfile
fastapi
uvicorn
python-multipart
joblib
```

---

# ğŸ¤ 3ï¸âƒ£ Prepare the Dataset

Add audio files organized like this:

```
dataset/beep/*.wav
dataset/clap/*.wav
dataset/noise/*.wav
```

âœ” Use `.wav` format
âœ” 1â€“3 seconds recommended
âœ” The label = the folder name

---

# ğŸ§  4ï¸âƒ£ Train the ML Model

Run the training script:

```bash
python src/train_model.py
```

This creates:

```
model/sound_model.pkl
```

If `model/` doesnâ€™t exist:

```bash
mkdir model
```

---

# ğŸš€ 5ï¸âƒ£ Run the FastAPI Server

Start the backend API:

```bash
uvicorn api.main:app --reload
```

API URL:

```
http://127.0.0.1:8000
```

---

# ğŸ”Š 6ï¸âƒ£ Make Predictions (Send Audio)

## Option A â€” Using curl

```bash
curl -X POST "http://127.0.0.1:8000/predict" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@your_audio.wav"
```

Response example:

```json
{
  "prediction": "clap"
}
```

---

## Option B â€” Python client example

```python
import requests

url = "http://127.0.0.1:8000/predict"
file = {'file': open("sound.wav", "rb")}
res = requests.post(url, files=file)

print(res.json())
```

Run:

```bash
python test_predict.py
```

---

# ğŸ›  Common Issues & Fixes

### âŒ `FileNotFoundError: ../model/sound_model.pkl`

â¡ï¸ Fix:

```bash
mkdir model
python src/train_model.py
```

---

### âŒ Missing python-multipart

```
RuntimeError: Form data requires "python-multipart"
```

â¡ï¸ Fix:

```bash
pip install python-multipart
```

---

### âŒ 404 at "/"

Not an issue â€” you can add this if you want:

```python
@app.get("/")
def home():
    return {"message": "Sound Classification API Running"}
```

---

# ğŸŒŸ Features

âœ” Audio feature extraction using Librosa
âœ” ML model using RandomForestClassifier
âœ” Easy-to-train dataset folder structure
âœ” FastAPI backend for audio inference
âœ” Upload audio via API or Python client
âœ” Clean, modular code

---

# ğŸ“œ License

This project is released under the **MIT License**.

---

# ğŸ™Œ Contributions

Pull requests are welcome â€” add more sound classes, UI frontends, or improved models.

---

# ğŸ‰ You're Ready!

Your sound classification ML system is now fully operational.
If you want, I can generate a **React frontend UI** that uploads audio and shows predictions.

```
```

